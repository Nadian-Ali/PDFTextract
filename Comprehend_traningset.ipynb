{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b074f6-d451-4904-92c8-6b8720a3d456",
   "metadata": {},
   "source": [
    "# Entity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc067bee-9c95-4c18-be46-3b22cd0af29f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "109421ad-3c28-495b-908d-0dce1f377914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (8.6.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.18.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (2.11.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (3.0.20)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.1.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from IPython) (0.4.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->IPython) (0.2.5)\n",
      "Requirement already satisfied: asttokens in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from stack-data->IPython) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from stack-data->IPython) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from stack-data->IPython) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from asttokens->stack-data->IPython) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b47a3a9-610a-47cf-9f72-320880ea14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2418aaa-b967-4a9d-b088-449be3507d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673571d-61be-4e7a-91bf-913cbeeb5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617cff4c-6245-45c7-8130-6809566af63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20a5175-4016-4933-8bd5-59d51cbacfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.layout    import LAParams#, LTTextBox\n",
    "from pdfminer.pdfpage   import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout    import LTTextBoxHorizontal,LTTextLine,LTTextLineHorizontal,LTLine,LTRect,LTFigure\n",
    "from pdfminer.utils     import open_filename\n",
    "from io                 import StringIO\n",
    "\n",
    "import textdistance   \n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pdfminer       \n",
    "import json\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d3310a-665f-42c7-9f62-aba67d8dd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d063d65-655b-49de-b09d-34716fd12e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NER.dataloader import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6584ebc0-8196-441a-b8b9-2d4128d3d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85590ae-b434-44ed-9b2c-68a544afa888",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecf1d7-c7fb-499d-88b0-abf61d563e4b",
   "metadata": {},
   "source": [
    "# Input data:\n",
    "\n",
    "we have data for training seven custom entities \n",
    "each one has two kinds of data:\n",
    "\n",
    "1. Annotations\n",
    "2. Documents\n",
    "\n",
    "**Annotations:** this is text file with comma separated lines, first part is an annotations or text and the next part is the entity name\n",
    "**Documents:** are parts of the pdfs that contain the annotation and need to be recognized. this is folder with many files. Each file is a piece of text that includes one or more annotations. \n",
    "\n",
    "\n",
    "context : This big body is going to see a scientific place\n",
    "            Annotations : big boy, entity name: student\n",
    "            Annotation  : scientific place : school \n",
    "            \n",
    "\n",
    "*we want to create training data with the following format:* **(context,(start_indx,end_index),entity-name)**<br>\n",
    "start and end indices are those  locate annotation in the context string \n",
    "\n",
    "\n",
    "# Process\n",
    "\n",
    "## Create records from one context \n",
    "1. Read one context\n",
    "2. Read the annotations file    \n",
    "    Create a record.      \n",
    "    The record is created by searching and locating any annotation in the context file.      \n",
    "    The record is a list of tuples.     \n",
    "\n",
    "## For all the documents:   \n",
    "Iterate ovell off the contexts in the a document \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cfe57-63cc-4d2d-a7df-b95e32da2ff0",
   "metadata": {},
   "source": [
    "## Define folder path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78df66a9-1b5a-4443-b5c8-bbec2a964a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'D:\\phoenixs3'\n",
    "\n",
    "model1_ann = 'D:\\phoenixs3\\Model-1\\Annotations\\model-1 (1).txt'  # lines 122 in this file is messed up!\n",
    "model2_ann = 'D:\\phoenixs3\\Model-2\\Annotations\\model-2 (1).txt'\n",
    "model3_ann = 'D:\\phoenixs3\\Model-3\\Annotations\\model-3 (2).txt'\n",
    "model4_ann = 'D:\\phoenixs3\\Model-4\\Annotations\\model-4.txt'\n",
    "model5_ann = 'D:\\phoenixs3\\Model-5\\Annotations\\model-5.txt'\n",
    "model6_ann = 'D:\\phoenixs3\\Model-6\\Annotations\\model-6.txt'\n",
    "model7_ann = 'D:\\phoenixs3\\Model-7\\Annotations\\model-7.txt'\n",
    "\n",
    "annotations = [model1_ann,model2_ann,model3_ann,model4_ann,model5_ann,model6_ann,model7_ann]\n",
    "# import all annotations as dataframe and remove the first row ! \n",
    "df1 = pd.read_csv(model1_ann, sep=\",\", names=['text','entity']); df1 = df1.drop(0)\n",
    "df2 = pd.read_csv(model2_ann, sep=\",\", names=['text','entity']); df2 = df2.drop(0)\n",
    "df3 = pd.read_csv(model3_ann, sep=\",\", names=['text','entity']); df3 = df3.drop(0)\n",
    "df4 = pd.read_csv(model4_ann, sep=\",\", names=['text','entity']); df4 = df4.drop(0)\n",
    "df5 = pd.read_csv(model5_ann, sep=\",\", names=['text','entity']); df5 = df5.drop(0)\n",
    "df6 = pd.read_csv(model7_ann, sep=\",\", names=['text','entity']); df6 = df6.drop(0)\n",
    "df7 = pd.read_csv(model1_ann, sep=\",\", names=['text','entity']); df7 = df7.drop(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5190b-1066-4798-8f2c-d09ca407993b",
   "metadata": {},
   "source": [
    "## load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12a8d32-6464-4c7c-a283-9df45bf6b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_model_docs\n",
    "\n",
    "model1_doc = 'D:\\phoenixs3\\Model-1\\Documents' # there are many docs in here \n",
    "model2_doc = 'D:\\phoenixs3\\Model-2\\Documents' # there are many docs in here \n",
    "model3_doc = 'D:\\phoenixs3\\Model-3\\Documents' # there are many docs in here \n",
    "model4_doc = 'D:\\phoenixs3\\Model-4\\Documents' # there are many docs in here \n",
    "model5_doc = 'D:\\phoenixs3\\Model-5\\Documents' # there are many docs in here \n",
    "model6_doc = 'D:\\phoenixs3\\Model-6\\Documents' # there are many docs in here \n",
    "model7_doc = 'D:\\phoenixs3\\Model-7\\Documents' # there are many docs in here \n",
    "\n",
    "document = {'model1_doc':model1_doc,\n",
    "        'model2_doc':model2_doc,\n",
    "        'model3_doc':model3_doc,\n",
    "        'model4_doc':model4_doc,\n",
    "        'model5_doc':model5_doc,\n",
    "        'model6_doc':model6_doc,\n",
    "        'model7_doc':model7_doc,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028b35fe-188c-4f70-8d3d-acbbb5c0b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_context_file_names(document,required_model):\n",
    "    \"\"\"\n",
    "    Reads one context file.\n",
    "    \n",
    "    Parameters:\n",
    "    document (dict): holds paths for all model\n",
    "    required_model (string): name of the model for which we want to extract data for \n",
    "  \n",
    "    Returns:\n",
    "    str: a string containing the text in the pdf   \n",
    "    \"\"\"\n",
    "    doc_path=[]\n",
    "    # print(required_model)\n",
    "    value = document[required_model]\n",
    "    # print(value)\n",
    "    docs = list(os.walk(value))    \n",
    "    for idx,file in enumerate(docs[0][2]):\n",
    "        file_name = os.path.join(value,file)\n",
    "        doc_path.append(file_name)\n",
    "    return doc_path\n",
    "\n",
    "#test the funciton \n",
    "# paths = get_model_context_file_names(document,'model1_doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf18f9f5-3867-4bc6-b5db-ec68855f1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_context_file(path):\n",
    "    with open(path,'r') as file:\n",
    "        context = file.read()\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d489d499-87b1-492b-9c6a-210a78361830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_from_context(context,df):\n",
    "    \"\"\"\n",
    "    Take a context string and all the entity sample with ther tag and generates (test, (start,end)index, entity-name)\n",
    "    e.g.: context:  this is show a big build that sells groceries at cheal price\n",
    "          text : sells groceries\n",
    "          entity-name: supermarket \n",
    "          output : (context, (30,45),supermarket)\n",
    "    Parameters:\n",
    "    context (str): holds the info which contains the data to search in \n",
    "    df (pandas frame): is the dataframe holding all text-entity pairs in two columns\n",
    "    Returns:\n",
    "    str: a list of tuples(contenxt,(start-index,end-index),entity-name)   \n",
    "    \"\"\"\n",
    "    record = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['text'] in context:\n",
    "            \n",
    "            text = row['text'].strip()\n",
    "            entity_name = row['entity']\n",
    "            text_length = len(text)\n",
    "            start_index = context.find(text)\n",
    "            end_index = start_index + text_length\n",
    "            record.append((context,(start_index,end_index),entity_name))\n",
    "    # clean the records: \n",
    "    return record\n",
    "        \n",
    "\n",
    "# #test function  \n",
    "# ##get path\n",
    "# paths = get_model_context_file_names(document,'model1_doc')\n",
    "\n",
    "# ## get context \n",
    "# a_context = read_context_file(paths[0])\n",
    "# ##get records \n",
    "# records = get_records_from_context(a_context,df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5074d-e189-421e-b42b-4f9f64c97c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c4bbb4-a6fe-4b9f-98ed-2eaaef3bc1a6",
   "metadata": {},
   "source": [
    "## Spacy format\n",
    "\n",
    "There is another way to create the dataset.    \n",
    "Since for one context, we have several entities inside we can go like :    \n",
    "\n",
    "#### Example 1\n",
    "<code style='color:yellow'>\n",
    "[\n",
    "    ('I want apples', {'entities': [(2, 5, 'COMMAND'), (7, 12, 'FRUIT')]})\n",
    "]\n",
    "\n",
    "</code>\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "<code style='color:yellow'>\n",
    "train_data=[\n",
    "    ('Who is Nishanth?', {\n",
    "        'entities': [(7, 15, 'PERSON')]\n",
    "    }),\n",
    "     ('Who is Kamal Khumar?', {\n",
    "        'entities': [(7, 19, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468f2bbc-1ddd-422b-9b64-b083ab944fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_Spacyformat(context,df):\n",
    "\n",
    "    entities = []\n",
    "    record = []\n",
    "    for index, row in df.iterrows():\n",
    "        # print(row['text'])\n",
    "        if row['text'] in context:\n",
    "            text = row['text'].strip()\n",
    "            entity_name = row['entity']\n",
    "            text_length = len(text)\n",
    "            start_index = context.find(text)\n",
    "            end_index = start_index + text_length\n",
    "            entities.append((start_index,end_index,entity_name))\n",
    "    record = [(context,{'entities':entities})]\n",
    "    # record = remove_overlap(record)\n",
    "    return record \n",
    "\n",
    "# #test function  \n",
    "# ##get path\n",
    "# paths = get_model_context_file_names(document,'model4_doc')\n",
    "\n",
    "# ## get context \n",
    "# a_context = read_context_file(paths[10])\n",
    "# ##get records \n",
    "# records = get_records_Spacyformat(a_context,df4)\n",
    "# records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aea16fa-2578-4a39-9296-63e0da025328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350803d4-2e84-4484-8c7a-32a76259dc2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find similar context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05850ac-0aef-4405-891d-461d5e0e7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_duplicate_context(paths):\n",
    "    text = []\n",
    "    for idx,path in tqdm(enumerate(paths)):\n",
    "        # if idx<10:\n",
    "\n",
    "            context = read_context_file(path)\n",
    "            contenxt = re.sub('[.(?)-]', '', context)\n",
    "            text.append(context)\n",
    "            texts = pd.Series(text)\n",
    "\n",
    "    dropped = texts.drop_duplicates()\n",
    "    return dropped\n",
    "\n",
    "# contexts_test = remove_duplicate_context(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d8990c-84db-4690-a04b-503d0e0a3428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    309g a listed building repairs notice 309g no ...\n",
       "1    calderdale customer service and communications...\n",
       "2    works no 39m a tree preservation order no 39n ...\n",
       "3    none k an order revoking or modifying planning...\n",
       "4    309g a listed building repairs notice 309g no ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74bcb68f-f92c-4f81-8212-180cce235d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise_from_record(records): \n",
    "    # some times, there is text which is subset of another, 'boy is hamin' and 'this boy is hamin' \n",
    "    entities = records[0][1]['entities']\n",
    "    for idx,data in enumerate(entities):\n",
    "\n",
    "        current_s = data[0]\n",
    "        current_e = data[1]\n",
    "        # print('idx:',idx,(data[0],data[1]))\n",
    "        # print(current_s,current_e)  \n",
    "        for idx1,data1 in enumerate(entities):\n",
    "            if idx != idx1:\n",
    "                new_s = data1[0]\n",
    "                new_e = data1[1]\n",
    "                # print(new_s,new_e)  \n",
    "                if current_s >= new_s and  current_e <= new_e:\n",
    "                    try:\n",
    "                        records[0][1]['entities'].pop(idx)\n",
    "                    except: \n",
    "                        print(idx)\n",
    "                    \n",
    "    \n",
    "    return records\n",
    "                \n",
    "# # test function\n",
    "# paths = get_model_context_file_names(document,'model1_doc')\n",
    "\n",
    "# ## get context \n",
    "# a_context = read_context_file(paths[10])\n",
    "# ##get records\n",
    "# records = get_records_Spacyformat(a_context,df1.drop_duplicates())\n",
    "\n",
    "# if len(records)>0:\n",
    "#     clean_records = remove_noise_from_record(records)\n",
    "# # records\n",
    "# clean_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc39fdb-2fb0-4348-8272-9ab6b5cb6bbb",
   "metadata": {},
   "source": [
    "# Load training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9315ff-a2f8-46f5-b8e6-d1db0be812e5",
   "metadata": {},
   "source": [
    "## Select training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d16934-4288-456a-86c3-d2b7e4937b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_context = 'model7_doc'\n",
    "model = model7_ann "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f3dbb-a57c-4fff-8099-670a88d55ed8",
   "metadata": {},
   "source": [
    "## Get DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71ffe53f-3b25-4757-b53c-fe7c5f6e3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(model, sep=\",\", names=['text','entity'])\n",
    "DF = DF.drop(0) ; \n",
    "DF = DF.drop_duplicates() # path  model1_ann is defined above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64a64dce-0f20-4eae-9e65-a3245350dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172if there are any entries</td>\n",
       "      <td>17.2_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172if there are entries</td>\n",
       "      <td>17.2_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172 if there are any entries</td>\n",
       "      <td>17.2_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a how can copies of the entries be obtained</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>172a if there are any entries how can copies o...</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172 a how can copies of the entries be obtained</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1702 a how can copies of the entries be obtained</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>172a if there are entries a how can copies of ...</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>172 a copies of matters entered if any can be ...</td>\n",
       "      <td>17.2A_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b where can the register be inspected</td>\n",
       "      <td>17.2B_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>172b where can the register be inspected</td>\n",
       "      <td>17.2B_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>172 b where can the register be inspected</td>\n",
       "      <td>17.2B_QUESTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text          entity\n",
       "1                         172if there are any entries   17.2_QUESTION\n",
       "2                             172if there are entries   17.2_QUESTION\n",
       "3                        172 if there are any entries   17.2_QUESTION\n",
       "5         a how can copies of the entries be obtained  17.2A_QUESTION\n",
       "6   172a if there are any entries how can copies o...  17.2A_QUESTION\n",
       "7     172 a how can copies of the entries be obtained  17.2A_QUESTION\n",
       "8    1702 a how can copies of the entries be obtained  17.2A_QUESTION\n",
       "11  172a if there are entries a how can copies of ...  17.2A_QUESTION\n",
       "12  172 a copies of matters entered if any can be ...  17.2A_QUESTION\n",
       "14              b where can the register be inspected  17.2B_QUESTION\n",
       "15           172b where can the register be inspected  17.2B_QUESTION\n",
       "17          172 b where can the register be inspected  17.2B_QUESTION"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed209ea8-4c32-4726-9f2e-0cb8ac1d13cf",
   "metadata": {},
   "source": [
    "## Get paths to context docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f853985-c348-4804-83b4-322eac0c6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example path is:\n",
      "\n",
      " D:\\phoenixs3\\Model-7\\Documents\\17.1_QUESTION_0222c661-1edc-4929-bdec-ab056c1695ab_1\n"
     ]
    }
   ],
   "source": [
    "paths    = get_model_context_file_names(document,model_context)\n",
    "print('An example path is:\\n\\n',paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5352e83-feb7-4289-9db6-7f3e6646ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4782it [00:07, 644.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4782 context files reduced to 211 after removing duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contexts = remove_duplicate_context(paths)\n",
    "print(f\"{len(paths)} context files reduced to {contexts.shape[0]} after removing duplicates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0a5ef-b924-4fc5-ac11-5c5e8cbe3d90",
   "metadata": {},
   "source": [
    "## Create clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1000050-b1c9-4a4b-8f77-e08595790c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " now creating labesl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 359.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset    = []\n",
    "noentities = 0\n",
    "print(' now creating labesl...')\n",
    "# now we have records for one document and note we have to append to dataset\n",
    "\n",
    "for context in tqdm(contexts):\n",
    "        record  =  get_records_Spacyformat(context,DF)\n",
    "    # print( records[0][1]['entities'])\n",
    "    \n",
    "        if len(record)>0: clean_record = remove_noise_from_record(record)\n",
    "        if len(clean_record)>0:  dataset.append(clean_record)\n",
    "        else :\n",
    "            noentities+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0fdaf2f-eb59-4416-ac5f-b0f6f22b59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9ddf2f5-05de-405f-b839-371a944a25af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 if there are any entries\n",
      "a how can copies of the entries be obtained\n",
      "b where can the register be inspected\n",
      "environmental and pollution notices 18 what outstanding statutory or informal notices have been issued by the local authority under the environmental protection act 1990 or the control of pollution act 1974 this enquiry does not cover notices under part iia or part iii of the epa to which enquiries 37 or 313 apply\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "text = dataset[idx][0][0]\n",
    "text\n",
    "ent =  dataset[idx][0][1]['entities']\n",
    "for entity in ent: \n",
    "    sx = entity[0]\n",
    "    ex = entity[1]\n",
    "    tex = text[sx:ex]\n",
    "    print(tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3979e-8306-4946-86f9-f8fc4484f198",
   "metadata": {},
   "source": [
    "<h2 style='color:red'>Save dataset to csv </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "186f7b72-7e84-47a0-a457-5a7035da23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(columns = ['label','text'])\n",
    "for item in dataset:\n",
    "    ent = item[0][1]['entities']\n",
    "    text = item[0][0]\n",
    "    for entity in ent:\n",
    "        # print('\\n', entity[2])\n",
    "    # break\n",
    "        temp_df = pd.DataFrame(columns = ['label','text'],data=[[entity[2],item]])\n",
    "        data_df = pd.concat([data_df,temp_df], ignore_index=True)                  \n",
    "# data_df.to_csv('model_5entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "96d9cfe9-aaf6-4867-a467-4d8b16b10e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86956b1-fe51-49ec-b339-acd7eb19c984",
   "metadata": {},
   "source": [
    "<h2 style='color:red'>Save dataset in Json</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e44a70b8-25ca-446a-9cf8-5d06553f40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_model7.json','w') as file:\n",
    "    json.dump(dataset,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a45ddc2e-2462-4993-8359-fdd30633d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_model6.json','r') as file:\n",
    "    test_opn_Dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "245b433b-5efd-4127-b033-3573b43ead32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_opn_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdfa28a0-e6cc-4162-b606-61a42bcc6e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r london borough of local land charges richmond upon thames london borough of richmond upon thames informative the historic buildings and monumentscommissior also called english heritage also have power to issue building preservation notices for listed buildings in london boroughs and enquiry should also be made of them if appropriate in respect of 39m please contact at the trees team to ascertain the status of any tree preservation order revealed in the answer trees@richmondgovuk or 020 8891 1411 310 community infrastructure levy cil a is there a cil charging schedule yes ttp//wwwrichmondgovuk/community_infrastructure_levy b if yes do any of the following subsist in relation to the property or has a local authority decided to issue serve make or commence any of the following none c has any demand notice been suspended none d has the local authority received full or part payment of any cil liability none e has the local authority received any appeal against any of the above none f has a decision been taken to apply for a liability order none g has a liability order been granted none h have any other enforcement measures been taken none 311 conservation area do the following apply in relation to the property a the making of the area a conservation area before 31 august 1974 search reference 21_00287  ',\n",
       "  {'entities': []}]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_opn_Dataset[310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178cd82-0e51-45a0-bf2a-34cee43669a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a391fced-6d90-4267-8377-b581a868ccca",
   "metadata": {},
   "source": [
    "# Go for simple matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a05eae-450e-4dc7-b0b2-a9fed7ea3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a list of all entities that can be detected in a special formats \n",
    "# lets see if we can match them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7d6cb-35c9-4ee3-bdf1-50bddff94213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the entities \n",
    "we have a set of entities \n",
    "we want to search that in a set of lines ! \n",
    "\n",
    "\n",
    "ok ! this is a line ! if this is the complete entit or not! but, it should be anyway \n",
    "if it is mark it \n",
    "\n",
    "who are you seraching ? the df \n",
    "ok then the get a tag \n",
    "and :D\n",
    "\n",
    "#extract text from a page\n",
    "\n",
    "# match cases \n",
    "\n",
    "#report the answers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394fd920-d00b-42ab-a2f3-8a0df061e0c7",
   "metadata": {},
   "source": [
    "# Go for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2839a-b598-442d-8b42-1467ad6a8f94",
   "metadata": {},
   "source": [
    "## load Process entity dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0d96fc2-8cf4-4e5c-9d61-3cc7674d6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'D:\\phoenixs3'\n",
    "\n",
    "model1_ann = 'D:\\phoenixs3\\Model-1\\Annotations\\model-1 (1).txt'  # lines 122 in this file is messed up!\n",
    "model2_ann = 'D:\\phoenixs3\\Model-2\\Annotations\\model-2 (1).txt'\n",
    "model3_ann = 'D:\\phoenixs3\\Model-3\\Annotations\\model-3 (2).txt'\n",
    "model4_ann = 'D:\\phoenixs3\\Model-4\\Annotations\\model-4.txt'\n",
    "model5_ann = 'D:\\phoenixs3\\Model-5\\Annotations\\model-5.txt'\n",
    "model6_ann = 'D:\\phoenixs3\\Model-6\\Annotations\\model-6.txt'\n",
    "model7_ann = 'D:\\phoenixs3\\Model-7\\Annotations\\model-7.txt'\n",
    "\n",
    "annotations = [model1_ann,model2_ann,model3_ann,model4_ann,model5_ann,model6_ann,model7_ann]\n",
    "# import all annotations as dataframe and remove the first row ! \n",
    "df1 = pd.read_csv(model1_ann, sep=\",\", names=['text','entity']); df1 = df1.drop(0) ; df1 = df1.drop_duplicates()\n",
    "df2 = pd.read_csv(model2_ann, sep=\",\", names=['text','entity']); df2 = df2.drop(0) ; df2 = df2.drop_duplicates()\n",
    "df3 = pd.read_csv(model3_ann, sep=\",\", names=['text','entity']); df3 = df3.drop(0) ; df3 = df3.drop_duplicates()\n",
    "df4 = pd.read_csv(model4_ann, sep=\",\", names=['text','entity']); df4 = df4.drop(0) ; df4 = df4.drop_duplicates()\n",
    "df5 = pd.read_csv(model5_ann, sep=\",\", names=['text','entity']); df5 = df5.drop(0) ; df5 = df5.drop_duplicates()\n",
    "df6 = pd.read_csv(model7_ann, sep=\",\", names=['text','entity']); df6 = df6.drop(0) ; df6 = df6.drop_duplicates()\n",
    "df7 = pd.read_csv(model1_ann, sep=\",\", names=['text','entity']); df7 = df7.drop(0) ; df7 = df7.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a7f02ed1-3d47-49ff-b813-46b154f321da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dataframes \n",
    "ent_df = pd.concat([df1, df2, df3, df4, df5, df6, df7], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0857059c-8d60-4fbc-96a9-7332dfdb3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get access to one itentity\n",
    "text   = pdf_df.iloc[0]['text']\n",
    "entity = pdf_df.iloc[0]['entity']\n",
    "\n",
    "# print ('\\n text:   ', text,'\\n\\n', 'entity :' ,entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bac74c-d366-4b40-a896-ebeace04e1cd",
   "metadata": {},
   "source": [
    "## Extract PDF pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f00471b4-4c8a-490f-bed6-eff48c29ce52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 44....\n"
     ]
    }
   ],
   "source": [
    "from utils.extract_pdf_elements import pdfDF\n",
    "columns = ['page','item','BBOX','text','size','sx','sy','ex','ey']\n",
    "pdf = 'hart3.pdf'\n",
    "PDF = pdfDF(pdf,columns)\n",
    "pdf_df = PDF.get_DF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a2cf9c9-05e3-4141-845f-8009cf888c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query page for textlines\n",
    "def get_textlines_in_page(df = df, page=1):\n",
    "    df_temp = df.query(f\"page=={page} and item=='LTTextLineHorizontal'\")\n",
    "    return df_temp\n",
    "\n",
    "page = 2\n",
    "txt_lines = get_textlines_in_page(df = df, page = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7ebc5f2-9183-4755-8435-ec62c88f309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>item</th>\n",
       "      <th>BBOX</th>\n",
       "      <th>text</th>\n",
       "      <th>size</th>\n",
       "      <th>sx</th>\n",
       "      <th>sy</th>\n",
       "      <th>ex</th>\n",
       "      <th>ey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>23</td>\n",
       "      <td>LTTextLineHorizontal</td>\n",
       "      <td>(152.20000000000002, 806.133, 440.718999999999...</td>\n",
       "      <td>Law Society CON29 Enquiries of Local Authority...</td>\n",
       "      <td>[0, 0, 595, 841]</td>\n",
       "      <td>152.2</td>\n",
       "      <td>806.133</td>\n",
       "      <td>440.719</td>\n",
       "      <td>817.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>23</td>\n",
       "      <td>LTTextLineHorizontal</td>\n",
       "      <td>(40.800000000000004, 781.2834000000001, 108.84...</td>\n",
       "      <td>Property Address:</td>\n",
       "      <td>[0, 0, 595, 841]</td>\n",
       "      <td>40.8</td>\n",
       "      <td>781.2834</td>\n",
       "      <td>108.8472</td>\n",
       "      <td>789.0834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>23</td>\n",
       "      <td>LTTextLineHorizontal</td>\n",
       "      <td>(130.8, 781.2834000000001, 401.733, 789.083400...</td>\n",
       "      <td>Beech House, Ancells Business Park Harvest Cre...</td>\n",
       "      <td>[0, 0, 595, 841]</td>\n",
       "      <td>130.8</td>\n",
       "      <td>781.2834</td>\n",
       "      <td>401.733</td>\n",
       "      <td>789.0834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>23</td>\n",
       "      <td>LTTextLineHorizontal</td>\n",
       "      <td>(40.800000000000004, 763.5334000000001, 51.642...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 595, 841]</td>\n",
       "      <td>40.8</td>\n",
       "      <td>763.5334</td>\n",
       "      <td>51.642</td>\n",
       "      <td>771.3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>23</td>\n",
       "      <td>LTTextLineHorizontal</td>\n",
       "      <td>(40.800000000000004, 735.9334000000001, 62.484...</td>\n",
       "      <td>1.01</td>\n",
       "      <td>[0, 0, 595, 841]</td>\n",
       "      <td>40.8</td>\n",
       "      <td>735.9334</td>\n",
       "      <td>62.484</td>\n",
       "      <td>743.7334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     page                  item  \\\n",
       "2785   23  LTTextLineHorizontal   \n",
       "2787   23  LTTextLineHorizontal   \n",
       "2789   23  LTTextLineHorizontal   \n",
       "2791   23  LTTextLineHorizontal   \n",
       "2793   23  LTTextLineHorizontal   \n",
       "\n",
       "                                                   BBOX  \\\n",
       "2785  (152.20000000000002, 806.133, 440.718999999999...   \n",
       "2787  (40.800000000000004, 781.2834000000001, 108.84...   \n",
       "2789  (130.8, 781.2834000000001, 401.733, 789.083400...   \n",
       "2791  (40.800000000000004, 763.5334000000001, 51.642...   \n",
       "2793  (40.800000000000004, 735.9334000000001, 62.484...   \n",
       "\n",
       "                                                   text              size  \\\n",
       "2785  Law Society CON29 Enquiries of Local Authority...  [0, 0, 595, 841]   \n",
       "2787                                  Property Address:  [0, 0, 595, 841]   \n",
       "2789  Beech House, Ancells Business Park Harvest Cre...  [0, 0, 595, 841]   \n",
       "2791                                                  1  [0, 0, 595, 841]   \n",
       "2793                                               1.01  [0, 0, 595, 841]   \n",
       "\n",
       "         sx        sy        ex        ey  \n",
       "2785  152.2   806.133   440.719   817.133  \n",
       "2787   40.8  781.2834  108.8472  789.0834  \n",
       "2789  130.8  781.2834   401.733  789.0834  \n",
       "2791   40.8  763.5334    51.642  771.3334  \n",
       "2793   40.8  735.9334    62.484  743.7334  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_lines.iloc[3]['text']\n",
    "txt_lines.shape\n",
    "txt_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4864683b-5203-46a8-a1b7-50807b7d9158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101j building regulations approval\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text  = '1.01(j) building regulations approval'\n",
    "\n",
    "line = re.sub('[().-]', ' ', text)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4fb20-61aa-44a6-84c6-e25a1d76b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "# a = 0/\n",
    "\n",
    "# for page in range (44):\n",
    "# print(page)\n",
    "page = 23\n",
    "txt_lines = get_textlines_in_page(df = df, page = page)\n",
    "\n",
    "for index0, row in txt_lines.iterrows():\n",
    "    # print(row['text'])\n",
    "    # t = re.sub('[().-]', ' ', row['text'])\n",
    "    print(t)\n",
    "    for index1,ent in ent_df.iterrows():\n",
    "        text = ent['text']\n",
    "        print(text)\n",
    "        ent_text = ent['text']\n",
    "        entity    = ent['entity']\n",
    "        \n",
    "        if (t == ent_text) :\n",
    "            print(f\"found {bcolors.BOLD}{row['text']}{bcolors.ENDC} {bcolors.OKBLUE } in {bcolors.ENDC } {ent_text[:100]}{bcolors.OKBLUE } with entity type {bcolors.ENDC }{entity}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f50b03f-fcbb-4895-ac07-ea98558a6422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 planning and building regulation decisions ...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 planning and building decisions and pending...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101 Planning and building regulation decisions...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 planning and building regulation decisions ...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101 planning and building regulation decisions...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11 planning and building regulation decisions ...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11 which of the following relating to the prop...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11 planning and building regulation decisions ...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>101 planning and building decisions and pendin...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>101 planning and building regulation decisions...</td>\n",
       "      <td>1.1_QUESTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        entity\n",
       "0  11 planning and building regulation decisions ...  1.1_QUESTION\n",
       "1  11 planning and building decisions and pending...  1.1_QUESTION\n",
       "2  101 Planning and building regulation decisions...  1.1_QUESTION\n",
       "3  11 planning and building regulation decisions ...  1.1_QUESTION\n",
       "4  101 planning and building regulation decisions...  1.1_QUESTION\n",
       "5  11 planning and building regulation decisions ...  1.1_QUESTION\n",
       "6  11 which of the following relating to the prop...  1.1_QUESTION\n",
       "7  11 planning and building regulation decisions ...  1.1_QUESTION\n",
       "8  101 planning and building decisions and pendin...  1.1_QUESTION\n",
       "9  101 planning and building regulation decisions...  1.1_QUESTION"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85630202-35d0-4266-a924-68bce4e58aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_csv(pdf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a33d1-de87-4aa5-ba6c-186826d46b1d",
   "metadata": {},
   "source": [
    "# Go for spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6751c3e5-c107-44de-9c6a-c3560b2de147",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.3-cp310-cp310-win_amd64.whl (11.9 MB)\n",
      "     --------------------------------------- 11.9/11.9 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.6/181.6 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (1.23.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.0-py3-none-any.whl (48 kB)\n",
      "     -------------------------------------- 48.9/48.9 kB 415.3 kB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 94.7/94.7 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp310-cp310-win_amd64.whl (479 kB)\n",
      "     ------------------------------------- 479.4/479.4 kB 15.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (65.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.6/58.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 14.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, click, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.3 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.0 preshed-3.0.8 pydantic-1.10.2 smart-open-5.2.1 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 typer-0.7.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4f748827-064d-4285-a76a-712d408102d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "010120c9-0a44-469c-bebf-4eb314401795",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocBin() # create a DocBin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "75bff127-7255-401f-b83b-20ae3d657287",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
      "     ------------------------------------ 587.7/587.7 MB 466.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from en-core-web-lg==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (65.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.23.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali.nadian\\.conda\\envs\\pdfprocessor\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.4.1\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee099f35-6876-4437-b562-9bde7f4c4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eb5de4ca-be30-4a7f-bd03-3fdeb80876be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e187c462-063b-4af1-b24f-b3e9e9930fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19600"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9309cd94-ede0-4ac6-adbf-bec875a688f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 775.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 99/99 [00:00<00:00, 860.90it/s]\n"
     ]
    }
   ],
   "source": [
    "db = DocBin() # create a DocBin object\n",
    "Data_train = Data[:1000]\n",
    "Data_valid = Data[19501:]\n",
    "for text, annot in tqdm(Data_train): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\") # save the docbin object\n",
    "\n",
    "for text, annot in tqdm(Data_valid): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./valid.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "cff10265-81bc-4ff9-a198-4a916abf2b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccb431-4ea6-492c-ad7d-b2df29d3bcd6",
   "metadata": {},
   "source": [
    "# Test trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f0f187-417d-469d-86e6-558e8e7898f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ali.nadian\\\\Documents\\\\pythonProjects\\\\PDFTextract'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e701c6b-256e-442b-a7d1-f059d960f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c629b4-6c67-445a-959c-df54efaff184",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_ner(\"Antiretroviral therapy (ART) is recommended for all HIV-infected\\\n",
    "individuals to reduce the risk of disease progression.\\nART also is recommended \\\n",
    "for HIV-infected individuals for the prevention of transmission of HIV.\\nPatients \\\n",
    "starting ART should be willing and able to commit to treatment and understand the\\\n",
    "benefits and risks of therapy and the importance of adherence. Patients may choose\\\n",
    "to postpone therapy, and providers, on a case-by-case basis, may elect to defer\\\n",
    "therapy on the basis of clinical and/or psychosocial factors.\")\n",
    "\n",
    "colors = {\"PATHOGEN\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6adff454-6cbf-41a8-ae95-5b13da72d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    c a conservation area consent\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1.1C_QUESTION</span>\n",
       "</mark>\n",
       " None \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    d a certificate of lawfulness of existing use or development\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1.1D_QUESTION</span>\n",
       "</mark>\n",
       " None \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    e a certificate of lawfulness of  proposed use or development\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1.1E_QUESTION</span>\n",
       "</mark>\n",
       " None \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    f a certificate of lawfulness of proposed works for listed buildings\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1.1F_QUESTION</span>\n",
       "</mark>\n",
       " None \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    g a heritage partnership agreement\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1.1G_QUESTION</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "text = \"(c) a conservation area consent \\\n",
    "None \\\n",
    "(d) a certificate of lawfulness of existing use or development \\\n",
    "None \\\n",
    "(e) a certificate of lawfulness of  proposed use or development \\\n",
    "None \\\n",
    "f a certificate of lawfulness of proposed works for listed buildings \\\n",
    "None \\\n",
    "(g) a heritage partnership agreement\"\n",
    "line = re.sub('[.()-]', '', text)\n",
    "# print(line)\n",
    "doc = nlp_ner(line)\n",
    "\n",
    "colors = {\"1.1_QUESTION\": \"#F67DE3\", \"1.1A_QUESTION\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24545202-1a73-45b2-aa2d-b798e773456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c a conservation area consent None d a certificate of lawfulness of existing use or development None e a certificate of lawfulness of  proposed use or development None f a certificate of lawfulness of proposed works for listed buildings None g a heritage partnership agreement"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734be3a9-a53f-4c49-9319-feb48fe5c5d8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef806a7-7210-499b-9695-c648719a65b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NER/dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NER/dataloader.py \n",
    "\n",
    "# write you improts here \n",
    "\n",
    "\n",
    "class annotations():\n",
    "    \n",
    "    def __init__(self,file_path):\n",
    "        self.path = file_path \n",
    "        \n",
    "    def read_file(self):\n",
    "        '''\n",
    "        Read annotation file \n",
    "        each line is an annotations containing \n",
    "        (text,lable) \n",
    "        text is a words that represent the entity \n",
    "        creates: a list \n",
    "        '''\n",
    "        with open(self.path,'r') as file:\n",
    "            \n",
    "            self.raw_file = file.readlines()\n",
    "   \n",
    "    def sort_annotations(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.read_file()\n",
    "        self.raw_file = self.raw_file[1:]\n",
    "        self.sorted_data = [(\n",
    "                          annotation[1].strip().split(',')[:-1][0],\n",
    "                          annotation[1].strip().split(',')[-1]\n",
    "                         ) for annotation in self.raw_file] \n",
    "        return self.sorted_data\n",
    "    \n",
    "def sum_a(a:int,b:int):\n",
    "    \n",
    "    return a+b\n",
    "\n",
    "# if __name__=='__main__':    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06a95c3-6276-49d3-a78f-8cd26bfd19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NER.dataloader import sum_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e885ce-8dbb-43e9-88bc-f68fb925aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd232b-6ae3-4fa1-910f-6a2b097a7c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63360ef-1e2b-4d0d-8b3f-43526fa629f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"11 planning and building regulation decisions and pending applications which of the following relating to the property have been granted issued or refused or where applicable are the subject of pending applications or agreements\",1.1_QUESTION\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82522c2-f47f-4e18-a9d3-10d21b38ee34",
   "metadata": {},
   "source": [
    "# Unit test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11852e40-97e0-42b3-8d20-781f68beb6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# %%writefile tests/loader_test.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "# import functions to be tested \n",
    "from NER.dataloader import sum_a\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "    def test_list_int(self):\n",
    "        \"\"\"\n",
    "        Test that it can sum a list of integers\n",
    "        \"\"\"\n",
    "        a = 10 \n",
    "        b = 10 \n",
    "        result = sum_a(a,b)\n",
    "        self.assertEqual(result, 20) , 'bad output'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "      unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "618aede4-ae81-46e5-b202-84e4b94bd6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
    "    'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
    "    'rating': [4, 4, 3.5, 15, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0c70304-3a66-4397-b481-b5dfc366b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>style</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yum Yum</td>\n",
       "      <td>cup</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yum Yum</td>\n",
       "      <td>cup</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indomie</td>\n",
       "      <td>cup</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indomie</td>\n",
       "      <td>pack</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indomie</td>\n",
       "      <td>pack</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand style  rating\n",
       "0  Yum Yum   cup     4.0\n",
       "1  Yum Yum   cup     4.0\n",
       "2  Indomie   cup     3.5\n",
       "3  Indomie  pack    15.0\n",
       "4  Indomie  pack     5.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9c677a5-1014-4b0e-9823-4a074cfc8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a7ec04-f970-41f3-b732-8186f32b014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(a):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m (idx)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idxt,item2 \u001b[38;5;129;01min\u001b[39;00m (a):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m dixt:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (a[idx] \u001b[38;5;241m==\u001b[39m a[idxt]):\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDFprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3198639205a59986fb10b9b92445be40a4b7c0ad1c630a81a3bd417444ec3644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
